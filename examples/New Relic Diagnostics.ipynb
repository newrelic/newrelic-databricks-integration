{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Notebook setup\n",
    "\n",
    "Run the next cell once to initialize the notebook parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"target_catalog\", \"my-catalog\", \"UC Catalog\")\n",
    "dbutils.widgets.text(\"target_schema\", \"my-schema\", \"UC Schema\")\n",
    "dbutils.widgets.text(\"target_volume\", \"my-volume\", \"UC Volume\")\n",
    "dbutils.widgets.text(\"target_cluster\", \"my-cluster\", \"Cluster ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verify running processes\n",
    "\n",
    "Run the next cell to verify that the integration process is running and, if enabled, that the New Relic Infrastructure agent and Fluent Bit processes are running.\n",
    "\n",
    "The result should be something like the following.\n",
    "\n",
    "```text\n",
    "Integration process running .............. YES\n",
    "Infrastructure processes running ......... YES\n",
    "Fluent Bit process running ............... YES\n",
    "```\n",
    "\n",
    "If the integration process is not running, enable the [startup logs](https://github.com/newrelic/newrelic-databricks-integration/blob/main/docs/troubleshooting.md#enabling-startup-logs-when-deployed-to-a-cluster) and inspect the startup logs for errors. If the startup logs are missing or contain no errors, [collect the cluster init script logs](https://github.com/newrelic/newrelic-databricks-integration/blob/main/docs/troubleshooting.md#collecting-cluster-init-script-logs) and inspect the logs for errors.\n",
    "\n",
    "If the infrastructure process is enabled but not running or the Fluent Bit process is not running and infrastructure logs is enabled, [collect the cluster init script logs](https://github.com/newrelic/newrelic-databricks-integration/blob/main/docs/troubleshooting.md#collecting-cluster-init-script-logs) and inspect the logs for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "if [ \"$(ps -ef | grep '/databricks/driver/newrelic/newrelic-databricks-integration' | grep -v grep | wc -l)\" -eq 1 ]; then\n",
    "  echo \"Integration process running .............. YES\"\n",
    "else\n",
    "  echo \"Integration process running .............. NO\"\n",
    "fi\n",
    "\n",
    "if [ \"$(ps -ef | grep '/usr/bin/newrelic-infra' | grep -v grep | wc -l)\" -eq 2 ]; then\n",
    "  echo \"Infrastructure processes running ......... YES\"\n",
    "else\n",
    "  echo \"Infrastructure processes running ......... NO\"\n",
    "fi\n",
    "\n",
    "if [ \"$(ps -ef | grep '/opt/fluent-bit/bin/fluent-bit' | grep -v grep | wc -l)\" -eq 1 ]; then\n",
    "  echo \"Fluent Bit process running ............... YES\"\n",
    "else\n",
    "  echo \"Fluent Bit process running ............... NO\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verify connectivity\n",
    "\n",
    "Run the next cell to verify that your cluster can access New Relic. To verify that this test is successful, first confirm that the last line of the output is the following:\n",
    "\n",
    "```json\n",
    "{\"success\":true, \"uuid\":\"[RANDOM-UUID]\"}\n",
    "```\n",
    "\n",
    "Second, use the [query builder](https://one.newrelic.com/data-exploration/query-builder) to run the following NRQL:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM DatabricksTest\n",
    "```\n",
    "\n",
    "The result should be the following.\n",
    "\n",
    "| Timestamp      | Message                  |\n",
    "| -------------- | ------------------------ |\n",
    "| [DATE], [TIME] | Hello, Databricks world! |\n",
    "\n",
    "If the test is unsuccessful, work with your Databricks administrator to determine if proxy, firewall, or other network issues may be blocking access to New Relic. For details on New Relic endpoints, ports, IP ranges, and other networking requirements, please consult the [New Relic network traffic documentation](https://docs.newrelic.com/docs/new-relic-solutions/get-started/networks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "echo '[{\"eventType\": \"DatabricksTest\", \"message\": \"Hello, Databricks world!\"}]' | \\\n",
    "  gzip | \\\n",
    "  curl -v \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -H \"Accept: application/json\" \\\n",
    "    -H \"Api-Key: $NEW_RELIC_LICENSE_KEY\" \\\n",
    "    -H \"Content-Encoding: gzip\" \\\n",
    "    https://insights-collector.newrelic.com/v1/accounts/$NEW_RELIC_ACCOUNT_ID/events \\\n",
    "    --data-binary \\\n",
    "    @-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Copy Databricks Integration startup logs to a volume\n",
    "\n",
    "Ensure that the [DBFS root](https://docs.databricks.com/aws/en/dbfs/#what-is-the-dbfs-root) is enabled for your account and workspace, and the Databricks Integration is configured to [copy startup logs](https://github.com/newrelic/newrelic-databricks-integration/blob/main/docs/troubleshooting.md#copying-startup-logs-to-dbfs). Then, use the notebook widget parameters to set the catalog, schema, and volume to the volume where the startup logs should be copied and run the next cell to copy the startup logs to the selected volume.\n",
    "\n",
    "When the files were copied successfully, the message \"Files copied!\" will be displayed in the cell output. In this case, navigate to the volume where you copied the startup logs to and use the appropriate controls to view or download the log files.\n",
    "\n",
    "If the files could not be copied, an error message will be displayed in the cell output. In this case, try [sending your startup logs to New Relic Logs](https://github.com/newrelic/newrelic-databricks-integration/blob/main/docs/troubleshooting.md#sending-startup-logs-to-new-relic-logs) instead.\n",
    "\n",
    "**NOTE:** Copying startup logs requires that access to the [DBFS root](https://docs.databricks.com/aws/en/dbfs/#what-is-the-dbfs-root) is enabled for your account and workspace. If access to the DBFS root is not enabled, startup logs will not be copied to DBFS and running the cell below will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "schema = dbutils.widgets.get(\"target_schema\")\n",
    "volume = dbutils.widgets.get(\"target_volume\")\n",
    "\n",
    "dbutils.fs.cp(\"dbfs:/tmp/newrelic-databricks-integration.log\", f\"/Volumes/{catalog}/{schema}/{volume}\")\n",
    "dbutils.fs.cp(\"dbfs:/tmp/newrelic-databricks-integration-startup-checks.log\", f\"/Volumes/{catalog}/{schema}/{volume}\")\n",
    "dbutils.fs.cp(\"dbfs:/tmp/newrelic-databricks-start-integration.sh.stdout.log\", f\"/Volumes/{catalog}/{schema}/{volume}\")\n",
    "dbutils.fs.cp(\"dbfs:/tmp/newrelic-databricks-start-integration.sh.stderr.log\", f\"/Volumes/{catalog}/{schema}/{volume}\")\n",
    "\n",
    "print(\"Files copied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Find the latest init script directories\n",
    "\n",
    "Ensure that [compute log delivery](https://docs.databricks.com/aws/en/compute/configure#compute-log-delivery) is configured to delivery cluster logs to a _Unity Catalog volume_ and restart the cluster if necessary. Then run this cell to locate the latest set of init script directories. Use the links generated in the cell output to navigate directly to the init script folders for the driver and worker nodes.\n",
    "\n",
    "**NOTE:** The init script directories are listed in descending order based on the modification time of the init script logs in each directory. There should be multiple links in the output. The init script log output for each startup attempt consists of one directory for the driver node and a separate, per-node directory for every worker in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "schema = dbutils.widgets.get(\"target_schema\")\n",
    "volume = dbutils.widgets.get(\"target_volume\")\n",
    "cluster_id = dbutils.widgets.get(\"target_cluster\")\n",
    "hours_ago = 1\n",
    "\n",
    "min_time = (int(time.time()) - (3600 * hours_ago)) * 1000\n",
    "dir = f\"/Volumes/{catalog}/{schema}/{volume}/{cluster_id}/init_scripts\"\n",
    "dirs = {}\n",
    " \n",
    "for f in dbutils.fs.ls(dir):\n",
    "    for f2 in dbutils.fs.ls(f\"{dir}/{f.name}\"):\n",
    "        dirs[f.name] = f2.modificationTime\n",
    "\n",
    "displayHTML(f\"<h2>Latest Init Script Directories (since {hours_ago} hours ago)</h2>\")\n",
    "\n",
    "for d in sorted(dirs.keys(), key=lambda x: dirs[x], reverse=True):\n",
    "    if dirs[d] > min_time:\n",
    "        dt = datetime.fromtimestamp(dirs[d] / 1000.0)\n",
    "        displayHTML(\n",
    "            f\"<h3>{dt.strftime('%Y-%m-%d %H:%M:%S %Z')}: <a href='/explore/data/volumes/{catalog}/{schema}/{volume}?volumePath={dir}/{d}'>{d}</a></h3>\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "New Relic Diagnostics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
