# Configuration

The Databricks Integration is configured using the [`config.yml`](#configyml)
and/or environment variables. Additionally Databricks [authentication](./authentication.md) related
configuration parameters may also be set in a [Databricks configuration profile](https://docs.databricks.com/en/dev-tools/auth/config-profiles.html).
In all cases, where applicable, environment variables always take precedence.

**NOTE:** When [deploying the integration to a Databricks cluster](./installation.md#deploy-the-integration-to-a-databricks-cluster),
the [`config.yml`](#configyml) is auto-generated by the [init script](../init/cluster_init_integration.sh)
and is customized using [compute environment variables](https://docs.databricks.com/aws/en/compute/configure#environment-variables).
In most cases, the [init script](../init/cluster_init_integration.sh) does
**not need to be modified** in order to customize the [`config.yml`](#configyml).

## `config.yml`

All configuration parameters for the Databricks integration can be set using a
YAML file named `config.yml`. The default location for this file is
`configs/config.yml` relative to the current working directory when the
integration binary is executed. The supported configuration parameters are
listed below. See [`config.template.yml`](../configs/config.template.yml) to see
all the configuration parameters that can be set.

### Configuration Using Environment Variables

When [deploying the integration remotely](./installation.md#deploy-the-integration-remotely),
all configuration parameters that can be set using the `config.yml` can be
overriden using environment variables in the environment where the integration
will be run. For a given configuration parameter, the parameter can be set using
an environment variable with the name equal to the upper "snake-case" version of
the configuration parameter key. For example, to set the [`interval`](#interval)
configuration parameter using an environment variable, use the string `INTERVAL`
as the environment variable name and the number of seconds as the value.

Setting nested configuration parameters using environment variables is also
supported. The environment variable name corresponding to a nested configuration
parameter is equal to the name of the upper "snake-case" version of each
configuration paramter key joined together using the `_` character. For example,
to set the log level configuration parameter, use the string `LOG_LEVEL` as the
environment variable name and the desired log level as the value.

**NOTE:** When [deploying the integration to a Databricks cluster](./installation.md#deploy-the-integration-to-a-databricks-cluster)
configuration parameters should **not** be overriden using environment variables
in this way. Instead, use [compute environment variables](https://docs.databricks.com/aws/en/compute/configure#environment-variables)
to specify the environment variables documented in the section
["Deploy the integration to a Databricks cluster"](./installation.md#deploy-the-integration-to-a-databricks-cluster)
in the [installation documentation](./installation.md).

### `licenseKey`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| New Relic license key | string | Y | N/a |

This parameter specifies the [New Relic License Key (INGEST)](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#license-key)
that should be used to send generated telemetry.

The license key can also be specified using the `NEW_RELIC_LICENSE_KEY`
environment variable.

### `apiKey`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| New Relic user key | string | Y | N/a |

This parameter is required but currently unused. It should be set to a dummy
value.

The API key can also be specified using the `NEW_RELIC_API_KEY` environment
variable.

### `accountId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| New Relic account ID | number | Y | N/a |

This parameter specifies the [New Relic account ID](https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/account-id/)
that should be used to send generated events.

The account ID can also be specified using the `NEW_RELIC_ACCOUNT_ID`
environment variable.

### `region`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| New Relic region identifier | `US` / `EU` | N | `US` |

This parameter specifies which [New Relic region](https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/choose-your-data-center/#regions-availability)
that generated telemetry should be sent to.

### `interval`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Polling interval, in seconds | numeric | N | 60 |

This parameter specifies the interval at which the integration should poll for
data, in seconds.

### `runAsService`

This parameter should always be set to `true`.

### `pipeline`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the integration's data pipeline | YAML Mapping | N | N/a |

The integration retrieves, processes, and exports data to New Relic using
a data pipeline consisting of one or more receivers, a processing chain, and a
New Relic exporter. Various aspects of the data pipeline are configurable. This
element groups together the configuration parameters related to the
configuration of the data pipeline.

**NOTE:** The integration data pipeline configuration has no relation to the
[Databricks pipelines configuration](#pipelines) or to the collection of
[Databricks pipeline update metrics and event logs](./pipelines.md).

#### `receiveBufferSize`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Size of the buffer that holds items before processing | number | N | 500 |

This parameter specifies the size of the data pipeline buffer that holds
received items before being flushed through the processing chain and on to the
exporters. When this size is reached, the items in the buffer will be flushed
automatically.

#### `harvestInterval`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Harvest interval, in seconds | number | N | 60 |

This parameter specifies the interval, in seconds, at which the data pipeline
should automatically flush received items through the processing chain and on
to the exporters. Each time this interval is reached, the pipeline will flush
items even if the data pipeline buffer has not reached the size specified by the
[`receiveBufferSize`](#receivebuffersize) parameter.

#### `instances`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Number of concurrent data pipeline instances to run | number | N | 3 |

The integration retrieves, processes, and exports metrics to New Relic using
a data pipeline consisting of one or more receivers, a processing chain, and a
New Relic exporter. The integration can launch one or more "instances" of the
data pipeline to receive, process, and export data concurrently. Each "instance"
will be configured with the same processing chain and exporters and the
receivers will be spread across the available instances in a round-robin
fashion.

This parameter specifies the number of data pipeline instances to launch.

### `log`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to logging | YAML Mapping | N | N/a |

The integration uses the [logrus](https://pkg.go.dev/github.com/sirupsen/logrus)
package for logging. This element groups together the configuration parameters
related to logging.

#### `level`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Log level | `panic` / `fatal` / `error` / `warn` / `info` / `debug` / `trace`  | N | `warn` |

This parameter specifies the maximum severity of log messages to output with
`trace` being the least severe and `panic` being the most severe. For example,
at the default log level (`warn`), all log messages with severities `warn`,
`error`, `fatal`, and `panic` will be output but `info`, `debug`, and `trace`
will not.

#### `fileName`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Path to a file where log output will be written | string | N | `stderr` |

This parameter designates a file path where log output should be written. When
no path is specified, log output will be written to the standard error stream
(`stderr`).

### `databricks`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of Databricks configuration parameters | YAML Mapping | N | N/a |

This element groups together the configuration parameters used to configure the
Databricks collector. If this element is not specified, the Databricks collector
will not be run.

Note that this element is not required. It can be used with or without the
[`spark`](#spark) parent element.

#### `workspaceHost`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Databricks workspace instance name | string | Y | N/a |

This parameter specifies the [instance name](https://docs.databricks.com/en/workspace/workspace-details.html#workspace-instance-names-urls-and-ids)
of the target Databricks deployment for which data should be collected. This is
used by the integration when constructing the URLs for [Databricks ReST API](https://docs.databricks.com/api/workspace/introduction)
calls. Note that the value of this parameter _must not_ include the `https://`
prefix.

The workspace host can also be specified using the `DATABRICKS_HOST`
environment variable.

#### `accessToken`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Databricks personal access token | string | N | N/a |

When set, the integration will use [Databricks personal access token authentication](https://docs.databricks.com/en/dev-tools/auth/pat.html)
to authenticate [Databricks ReST API](https://docs.databricks.com/api/workspace/introduction)
calls with the value of this parameter as the Databricks [personal access token](https://docs.databricks.com/aws/en/dev-tools/auth/pat#create-personal-access-tokens-for-workspace-users).

The personal access token can also be supplied using the relevant
[environment variables and configuration fields](https://docs.databricks.com/aws/en/dev-tools/auth/env-vars)
supported by the [Databricks unified authentication](https://docs.databricks.com/aws/en/dev-tools/auth/unified-auth)
standard.

See the [authentication documentation](./authentication.md) for more details.

#### `oauthClientId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Databricks service principal client ID | string | N | N/a |

When set, the integration will [use service principal authentication via OAuth](https://docs.databricks.com/en/dev-tools/auth/oauth-m2m.html)
when making Databricks API calls. The value of this parameter will be used as
the OAuth client ID.

The service principal client ID can also be supplied using the relevant
[environment variables and configuration fields](https://docs.databricks.com/aws/en/dev-tools/auth/env-vars)
supported by the [Databricks unified authentication](https://docs.databricks.com/aws/en/dev-tools/auth/unified-auth)
standard.

See the [authentication documentation](./authentication.md) for more details.

#### `oauthClientSecret`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Databricks service principal secret | string | N | N/a |

When the [`oauthClientId`](#oauthclientid) is set, this parameter may be set to
specify the service principal secret associated with the [service principal](https://docs.databricks.com/en/admin/users-groups/service-principals.html#what-is-a-service-principal)
used when [using service principal authentication via OAuth](https://docs.databricks.com/en/dev-tools/auth/oauth-m2m.html).
The value of this parameter will be used as the OAuth client secret.

The service principal secret can also be supplied using the relevant
[environment variables and configuration fields](https://docs.databricks.com/aws/en/dev-tools/auth/env-vars)
supported by the [Databricks unified authentication](https://docs.databricks.com/aws/en/dev-tools/auth/unified-auth)
standard.

See the [authentication documentation](./authentication.md) for more details.

#### `azureMsiEnabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable Azure managed identity authentication | `true` / `false` | N | `false` |

When the [`azureClientId`](#azureclientid) is set, this parameter can be set
to `true` to authenticate using
[Azure managed identity authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi).
If the [`azureClientId`](#azureclientid) is set, but this parameter is not
specified or set to `false`, the integration will authenticate using
[Azure Entra service principal authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp).

The flag to configure Azure managed identity authentication (`ARM_USE_MSI`) can
also be supplied using the relevant
[environment variables](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi#environment)
and
[configuration profile fields](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi#profile).

See the [authentication documentation](./authentication.md) for more details.

#### `azureClientId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Client ID to use with Azure managed identity or Azure Entra service principal authentication | string | N | N/a |

When set, the integration will use either
[Azure managed identity authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi)
or
[Azure Entra service principal authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp),
depending on the value of the [`azureMsiEnabled`](#azuremsienabled) flag or
the associated Databricks environment variable or configuration field. The value
of this parameter will be used as Azure client ID.

The Azure client ID can also be supplied using the relevant
[environment variables](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi#environment)
and
[configuration profile fields](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi#profile).

See the [authentication documentation](./authentication.md) for more details.

#### `azureClientSecret`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Client secret to use with Azure Entra service principal authentication | string | N | N/a |

When the [`azureClientId`](#azureclientid) is set and
[`azureMsiEnabled`](#azuremsienabled) is missing or set to `false`, this
parameter may be set to specify the Azure Entra service principal client secret
to use with
[Azure Entra service principal authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp).

The Azure Entra service principal client secret can also be supplied using the
relevant
[environment variables](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#environment-variables)
and
[configuration profile fields](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#configuration-profiles).

See the [authentication documentation](./authentication.md) for more details.

#### `azureTenantId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Tenant ID to use with Azure Entra service principal authentication | string | N | N/a |

When the [`azureClientId`](#azureclientid) is set and
[`azureMsiEnabled`](#azuremsienabled) is missing or set to `false`, this
parameter may be set to specify the tenant ID to use with
[Azure Entra service principal authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp).

The tenant ID can also be supplied using the relevant
[environment variables](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#environment-variables)
and
[configuration profile fields](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#configuration-profiles).

See the [authentication documentation](./authentication.md) for more details.

#### `azureResourceId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Azure resource ID for your Azure Databricks workspace to use with Azure managed identity or Azure Entra service principal authentication | string | N | N/a |

When authenticating with
[Azure managed identity authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi)
or
[Azure Entra service principal authentication](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp),
this parameter may be set to specify the Azure resource ID of your Azure
Databricks workspace.

The Azure resource ID for your Azure Databricks workspace can also be supplied
using the relevant
[environment variables](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#environment-variables)
and
[configuration profile fields](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-sp#configuration-profiles).

See the [authentication documentation](./authentication.md) for more details.

**NOTE:** Before using the this configuration parameter, note that the
Databricks Azure
[documentation](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth/azure-mi#environment)
"recommends [specifying the workspace host (for example using the
[`workspaceHost`](#workspacehost) configuration parameter)] and explicitly
assigning the identity to the workspace." Using this configuration parameter
instead "requires Contributor or Owner permissions on the Azure resource, or a
custom role with specific Azure Databricks permissions."

#### `sqlStatementTimeout`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Timeout to use when executing SQL statements on a SQL warehouse, in seconds | number | N | 30 |

Certain telemetry collected by the Databricks collector requires the collector
to run SQL statements on a Databricks [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html).
This configuration parameter specifies the number of seconds to wait before
timing out a pending or running SQL query.

Currently, only [consumption and cost data](./billable-usage.md) collection
requires executing SQL statements on a [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html).

#### `usage`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks consumption and cost data | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks [consumption and cost data](./billable-usage.md).

##### `usage` > `enabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable automatic collection of Databricks consumption and cost data | `true` / `false` | N | `true` |

By default, when the Databricks collector is enabled, it will automatically
collect [consumption and cost data](./billable-usage.md).

This flag can be used to disable the collection of [consumption and cost data](./billable-usage.md)
by the Databricks collector. This may be useful when running multiple instances
of the Databricks Integration. In this scenario, [consumption and cost data](./billable-usage.md)
collection should _only_ be enabled on a single instance of the integration.
Otherwise, [consumption and cost data](./billable-usage.md) will be recorded
more than once in New Relic, affecting consumption and cost calculations.

**NOTE:**
* The collection of consumption and cost data is _disabled_ by default when the
  integration is [deployed to a Databricks cluster](./installation.md#deploy-the-integration-to-a-databricks-cluster).
* When the collection of consumption and cost data is enabled, a [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html)
  ID **must** be specified or the integration will _fail_ to start.

##### `usage` > `warehouseId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| ID of a SQL warehouse on which to run usage-related SQL statements | string | conditional | N/a |

The ID of a [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html)
on which to run the SQL statements used to collect [consumption and cost data](./billable-usage.md).

A [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html)
ID _must_ be specified when the collection of [consumption and cost data](./billable-usage.md)
is [enabled](#usage--enabled) or the integration will _fail_ to start.

##### `usage` > `includeIdentityMetadata`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable inclusion of identity related metadata with Databricks consumption and cost data | `true` / `false` | N | `false` |

When the collection of Databricks [consumption and cost data](./billable-usage.md)
is [enabled](#usage--enabled), the Databricks collector can include
several pieces of identifying information along with the consumption and cost
data.

By default, when the collection of Databricks [consumption and cost data](./billable-usage.md)
is [enabled](#usage--enabled), the Databricks collector will _not_
collect such data as it may be personally identifiable. This flag can be used to
enable the inclusion of the identifying information.

When [enabled](#usage--enabled), the following values are included.

* The [`run_as` identity](https://docs.databricks.com/aws/en/admin/system-tables/billing#run_as-identities)
  included in the [identity metadata](https://docs.databricks.com/aws/en/admin/system-tables/billing#identity-metadata-reference)
  returned from usage records in the [billable usage system table](https://docs.databricks.com/en/admin/system-tables/billing.html).
* The identity of the cluster creator for each usage record for billable usage
  attributed to non-serverless compute.
* The single user name for each usage record for billable usage attributed to
  non-serverless compute configured for [single-user (dedicated) access mode](https://docs.databricks.com/en/compute/configure.html#access-mode).
* The identity of the warehouse creator for each usage record for billable usage
  attributed to [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html) compute.
* The identity of the user or service principal used to run jobs for each query
  result collected by [job cost](./billable-usage.md#job-costs) queries.

##### `usage` > `runTime`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Time of day (as `HH:mm:ss`) at which to run consumption and cost data collection | string with format `HH:mm:ss` | N | `02:00:00` |

This parameter specifies the time of day at which the collection of
[consumption and cost data](./billable-usage.md) will occur. The value must be
of the form `HH:mm:ss` where `HH` is the `0`-padded 24-hour clock hour
(`00` - `23`), `mm` is the `0`-padded minute (`00` - `59`) and `ss` is the
`0`-padded second (`00` - `59`). For example, `09:00:00` is the time 9:00 AM and
`23:30:00` is the time 11:30 PM.

The time will _always_ be interpreted according to the UTC time zone. The time
zone can not be configured. For example, to specify that the integration should
be run at 2:00 AM EST (-0500), the value `07:00:00` should be specified.

##### `usage` > `optionalQueries`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of flags used to selectively enable or disable optional usage queries | YAML Mapping | N | N/a |

When the collection of Databricks [consumption and cost data](./billable-usage.md)
is [enabled](#usage--enabled), the Databricks collector will always
collect [billable usage data](./billable-usage.md#billable-usage-data) and
[list pricing data](./billable-usage.md#list-pricing-data) on every run. In
addition, by default, the Databricks collector will also run all
[job cost](./billable-usage.md#job-costs) queries on every run. However, the
latter behavior can be configured using a set of flags specified with this
configuration property to selectively enable or disable the [job cost](./billable-usage.md#job-costs)
queries. Each flag is specified using a property with the query ID as the name
of the property and `true` or `false` as the value of the property. The
following flags are supported.

* [`jobs_cost_list_cost_per_job_run`](./billable-usage.md#list-cost-per-job-run)
* [`jobs_cost_list_cost_per_job`](./billable-usage.md#list-cost-per-job)
* [`jobs_cost_frequent_failures`](./billable-usage.md#list-cost-of-failed-job-runs-for-jobs-with-frequent-failures)
* [`jobs_cost_most_retries`](./billable-usage.md#list-cost-of-repaired-job-runs-for-jobs-with-frequent-repairs)

For example, to enable the query ["list cost per job run"](./billable-usage.md#list-cost-per-job-run)
and the query ["list cost per job"](./billable-usage.md#list-cost-per-job) but
disable the query ["list cost of failed job runs for jobs with frequent failures"](./billable-usage.md#list-cost-of-failed-job-runs-for-jobs-with-frequent-failures)
and the query ["list cost of repaired job runs for jobs with frequent repairs"](./billable-usage.md#list-cost-of-repaired-job-runs-for-jobs-with-frequent-repairs),
the following configuration would be specified.

```yaml
  optionalQueries:
    jobs_cost_list_cost_per_job_run: true
    jobs_cost_list_cost_per_job: true
    jobs_cost_frequent_failures: false
    jobs_cost_most_retries: false
```

#### `jobs`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks job telemetry | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks [job telemetry](./jobs.md).

##### `jobs` > `runs`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks job run metrics | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks [job run metrics](./jobs.md).

###### `jobs` > `runs` > `enabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable automatic collection of job run metrics | `true` / `false` | N | `true` |

By default, when the Databricks collector is enabled, it will automatically
collect [job run metrics](./jobs.md).

This flag can be used to disable the collection of [job run metrics](./jobs.md)
by the Databricks collector. This may be useful when running multiple instances
of the Databricks Integration against the same Databricks [workspace](https://docs.databricks.com/en/getting-started/concepts.html#accounts-and-workspaces).
In this scenario, the collection of Databricks [job run metrics](./jobs.md)
should _only_ be enabled on a single instance of the integration. Otherwise,
[job run metrics](./jobs.md) will be recorded more than once in New Relic,
making troubleshooting challenging and affecting product features that use [job run events](./jobs.md#job-run-events),
such as [dashboards](https://docs.newrelic.com/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/)
and [alerts](https://docs.newrelic.com/docs/alerts/overview/).

###### `jobs` > `runs` > `startOffset`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Offset from the current time, in seconds, to use for calculating the earliest job run start time to match when listing job runs | number | N | 86400 (1 day) |

This parameter specifies an offset, in seconds, that can be used to tune the
performance of the collection of [job run metrics](./jobs.md) by limiting the
number of job runs to return when listing job runs to job runs with start times
greater than the current time at collection minus the offset.

See the section [`startOffset` Configuration](./jobs.md#startoffset-configuration)
for more details.

#### `pipelines`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks pipeline telemetry | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks Lakeflow Declarative Pipeline [telemetry](./pipelines.md).

##### `pipelines` > `metrics`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks pipeline update metrics | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks Lakeflow Declarative Pipeline [update metrics](./pipelines#pipeline-update-metrics).

###### `pipelines` > `metrics` > `enabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable automatic collection of Databricks Lakeflow Declarative Pipeline update metrics | `true` / `false` | N | `true` |

By default, when the Databricks collector is enabled, it will automatically
collect Databricks Lakeflow Declarative Pipeline [update metrics](./pipelines.md#pipeline-update-metrics).

This flag can be used to disable the collection of Databricks Lakeflow
Declarative Pipeline [update metrics](./pipelines.md#pipeline-update-metrics) by
the Databricks collector. This may be useful when running multiple instances of
the Databricks Integration against the same Databricks [workspace](https://docs.databricks.com/en/getting-started/concepts.html#accounts-and-workspaces).
In this scenario, the collection of Databricks Lakeflow Declarative Pipeline
[update metrics](./pipelines.md#pipeline-update-metrics) should _only_ be
enabled on a single instance of the integration. Otherwise, Databricks Lakeflow
Declarative Pipeline [update metrics](./pipelines.md#pipeline-update-metrics)
will be recorded more than once in New Relic, making troubleshooting challenging
and affecting product features that use [pipeline update events](./pipelines.md#pipeline-update-events),
such as [dashboards](https://docs.newrelic.com/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/)
and [alerts](https://docs.newrelic.com/docs/alerts/overview/).

###### `pipelines` > `metrics` > `startOffset`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Offset from the current time, in seconds, to use for calculating the earliest pipeline event timestamp to match when listing pipeline events | number | N | 86400 (1 day) |

This parameter specifies an offset, in seconds, that can be used to tune the
performance of the collection of pipeline [update metrics](./pipelines.md#pipeline-update-metrics)
by limiting the number of pipeline events to return when listing pipeline
events to pipeline events with start times greater than the current time at
collection minus the offset.

See the section [`startOffset` Configuration](./pipelines.md#pipeline-startoffset-configuration)
for more details.

###### `pipelines` > `metrics` > `intervalOffset`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Offset from the current time, in seconds, to use for delaying the collection of pipeline events to account for lag in the list pipeline events API | number | N | 5 |

This parameter specifies an offset, in seconds, that the Databricks collector
will use to delay collection of pipeline events in order to account for
potential lag in the [list pipeline events endpoint](https://docs.databricks.com/api/workspace/pipelines/listpipelineevents)
of the [Databricks ReST API](https://docs.databricks.com/api/workspace/introduction).

See the section [`intervalOffset` Configuration](./pipelines.md#pipeline-intervaloffset-configuration)
for more details.

##### `pipelines` > `logs`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks pipeline event logs | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks Lakeflow Declarative Pipeline [event logs](./pipelines#pipeline-event-logs).

###### `pipelines` > `logs` > `enabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable automatic collection of Databricks Lakeflow Declarative Pipeline event logs | `true` / `false` | N | `true` |

By default, when the Databricks collector is enabled, it will automatically
collect Databricks Lakeflow Declarative Pipeline [event logs](./pipelines#pipeline-event-logs).

This flag can be used to disable the collection of Databricks Lakeflow
Declarative Pipeline [event logs](./pipelines.md#pipeline-event-logs) by the
Databricks collector. This may be useful when running multiple instances of the
Databricks Integration against the same Databricks [workspace](https://docs.databricks.com/en/getting-started/concepts.html#accounts-and-workspaces).
In this scenario, the collection of Databricks Lakeflow Declarative Pipeline
[event logs](./pipelines.md#pipeline-event-logs) should _only_ be enabled on a
single instance of the integration. Otherwise, duplicate New Relic log entries
will be created for each Databricks Lakeflow Delcarative Pipeline [event log](./pipelines.md#pipeline-event-logs)
entry, making troubleshooting challenging and affecting product features that
use [pipeline event logs](./pipelines.md#pipeline-event-logs), such as
[dashboards](https://docs.newrelic.com/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/)
and [alerts](https://docs.newrelic.com/docs/alerts/overview/).

#### `queries`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks query telemetry | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks query telemetry.

##### `queries` > `metrics`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of configuration parameters related to the collection of Databricks query metrics | YAML Mapping | N | N/a |

This element groups together the configuration parameters related to the
collection of Databricks [query metrics](./queries.md).

###### `queries` > `metrics` > `enabled`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable automatic collection of Databricks query metrics | `true` / `false` | N | `true` |

By default, when the Databricks collector is enabled, it will automatically
collect Databricks [query metrics](./queries.md).

This flag can be used to disable the collection of [query metrics](./queries.md)
by the Databricks collector. This may be useful when running multiple instances
of the Databricks Integration against the same Databricks [workspace](https://docs.databricks.com/en/getting-started/concepts.html#accounts-and-workspaces).
In this scenario, the collection of Databricks [query metrics](./queries.md)
should _only_ be enabled on a single instance of the integration. Otherwise,
[query metrics](./queries.md) will be recorded more than once in New Relic,
making troubleshooting challenging and affecting product features that use
[query metrics](./queries.md), such as [dashboards](https://docs.newrelic.com/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/)
and [alerts](https://docs.newrelic.com/docs/alerts/overview/).

###### `queries` > `metrics` > `includeIdentityMetadata`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Flag to enable inclusion of identity related metadata with Databricks query metrics | `true` / `false` | N | `false` |

When the collection of Databricks [query metrics](./queries.md) is [enabled](#queries--metrics--enabled),
the Databricks collector can include several pieces of identifying information
along with the [query metrics](./queries.md).

By default, when the collection of Databricks [query metrics](./queries.md)
is [enabled](#queries--metrics--enabled), the Databricks collector will
_not_ collect such data as it may be personally identifiable. This flag can be
used to enable the inclusion of the identifying information.

When [enabled](#queries--metrics--enabled), the following values are included.

* The ID of the user that executed the query
* The email address or username of the user that executed the query
* The ID of the user whose credentials were used to execute the query
* The email address or username whose credentials were used to execute the query
* The identity of the warehouse creator of the [SQL warehouse](https://docs.databricks.com/en/compute/sql-warehouse/index.html)
  where the query was executed

###### `queries` > `metrics` > `startOffset`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Offset from the current time, in seconds, to use for calculating the start of the time range used when listing the query history | number | N | 600 |

This parameter specifies an offset, in seconds, that the Databricks collector
will use to calculate the start time of the time range used to call the
[list query history API](https://docs.databricks.com/api/workspace/queryhistory/list).

See the section [`startOffset` Configuration](./queries.md#query-metrics-startoffset-configuration)
for more details.

###### `queries` > `metrics` > `intervalOffset`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| Offset from the current time, in seconds, to use for delaying the collection of queries to account for lag in the list query history API | number | N | 5 |

This parameter specifies an offset, in seconds, that the Databricks collector
will use to delay collection of queries in order to account for potential lag in
the [list query history API](https://docs.databricks.com/api/workspace/queryhistory/list)
of the [Databricks ReST API](https://docs.databricks.com/api/workspace/introduction).

See the section [`intervalOffset` Configuration](./queries.md#query-metrics-intervaloffset-configuration)
for more details.

###### `queries` > `metrics` > `maxResults`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The maximum number of results to return per page on query history API calls | number | N | 100 |

This parameter specifies the maximum number of results to return per page when
calling the [list query history API](https://docs.databricks.com/api/workspace/queryhistory/list).

**NOTE:**
For performance reasons, since the integration requests query metrics for each
query using the [list query history API](https://docs.databricks.com/api/workspace/queryhistory/list),
it is recommended to keep the `maxResults` parameter at its default value in
most cases.

### `spark`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of Spark configuration parameters | YAML Mapping | N | N/a |

This element groups together the configuration parameters to configure the Spark
collector. If this element is not specified, the Spark collector will not be
run.

Note that this element is not required. It can be used with or without the
[`databricks`](#databricks) parent element.

#### `webUiUrl`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The URL of the Web UI of an application on the Spark deployment to monitor | string | N | N/a |

This parameter specifies the URL of the [Web UI](https://spark.apache.org/docs/latest/web-ui.html) of an
application running on the Spark deployment to monitor. The value should be of
the form `http[s]://<hostname>:<port>` where `<hostname>` is the hostname of the
Spark deployment to monitor and `<port>` is the port number of the Spark
application's Web UI (typically 4040, or 4041, 4042, and so forth if more than
one application is running on the same host).

Note that the value must not contain a path. The path of the [Spark ReST API](https://spark.apache.org/docs/latest/monitoring.html#rest-api)
endpoints will automatically be appended.

#### `clusterManager`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The cluster manager on which the Spark engine is running | string | N | `standalone` |

This parameter specifies an identifier that indicates the cluster manager on
which the Spark engine is running.

The valid values for this parameter are `standalone` and `databricks`. When
this parameter is set to `databricks`, metrics are decorated with additional
attributes (such as the [workspace name, URL, and ID](https://docs.databricks.com/en/workspace/workspace-details.html#workspace-instance-names-urls-and-ids)).
When this parameter is not set, or is set to `standalone`, no additional
telemetry or attributes are collected.

**NOTE**: The `databricks` value is reserved for
[local cluster deployments](./installation.md#deploy-the-integration-to-a-databricks-cluster)
and is automatically configured via the
[cluster-scoped init script](../init/cluster_init_integration.sh). Spark metric
collection is not supported for remote deployments. The integration will fail
to start if this configuration parameter is set to `databricks` and the
integration is not installed on a Databricks cluster.

#### `databricks`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for the set of Spark on Databricks configuration parameters | YAML Mapping | N | N/a |

This element groups together the configuration parameters to configure the Spark
collector when the [`clusterManager`](#clustermanager) is set to `databricks`.
This configuration parameter and the configuration parameters it contains are
ignored if the [`clusterManager`](#clustermanager) is not set to `databricks`.

##### `clusterId`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The ID of the Databricks cluster on which the Spark deployment is running | string | N | N/a |

This configuration parameter specifies the ID of the Databricks
[cluster](https://docs.databricks.com/en/getting-started/concepts.html#cluster)
on which the Spark deployment is running.

**NOTE:**
* This configuration parameter is reserved for
[local cluster deployments](./installation.md#deploy-the-integration-to-a-databricks-cluster)
and is automatically configured via the
[cluster-scoped init script](../init/cluster_init_integration.sh). Spark metric
collection is not supported for remote deployments.
* The cluster ID for a Databricks cluster can be found by performing the
  following steps.

  1. Click "Compute" in the workspace sidebar
  1. Select the "All-purpose compute" or "Job compute" tab as appropriate
  1. Click on the name of the compute
  1. In the path portion of the URL, the cluster ID for the compute is the text
     after the path `/compute/clusters/`. For example, if the path is
     `/compute/clusters/0000-123456-abcdefgh`, the cluster ID is
     `0000-123456-abcdefgh`.

### `tags`

| Description | Valid Values | Required | Default |
| --- | --- | --- | --- |
| The parent element for a set of custom tags to add to all telemetry sent to New Relic | YAML Mapping | N | N/a |

This element specifies a set of custom [tags](https://docs.newrelic.com/docs/new-relic-solutions/new-relic-one/core-concepts/use-tags-help-organize-find-your-data/),
specified as key-value pairs, that will be added to _all_ telemetry sent to
New Relic.
